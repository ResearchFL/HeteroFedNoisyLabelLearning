Federated Learning (FL) aims to learn a common model from different clients while maintaining client data privacy, and it has gradually been applied to real-world applications (Li et al. 2020b; He et al. 2020, 2019). However, the presence of noisy and biased data in each local client severely degrades the generalization performance of FL models (Wang et al. 2022). Moreover, FL faces the challenge of heterogeneous noisy label distributions across clients, which further complicates the learning process.

Unlike Centralized Learning (CL) with noisy labels, FL confronts a unique situation where each client's noisy label distribution may differ due to variations in the true label distribution and personalized human labeling errors (Wei et al. 2022; Han et al. 2018; Yi et al. 2022). Figure 1(a) visually demonstrates this challenge, depicting significant differences in the distribution of noisy labels between client p and client q, despite sharing the same categories as the whole dataset on all clients. Consequently, the existence of heterogeneous noisy labels in FL severely impairs the generalization performance of FL models (Wang et al. 2022), leading to an even more pronounced impact on FL compared to CL with noisy labels. Therefore, there is a critical need for a novel label noise learning method tailored to FL to address this challenge effectively.

Numerous research efforts have been devoted to addressing noisy labels in CL (Cheng et al. 2021; Li, Socher, and Hoi 2019; Liu and Guo 2020; Natarajan et al. 2013; Wei et al. 2020). However, when directly applied to FL, these CL-based methods encounter significant challenges. For instance, learning a denoising model using local noisy label data on the client-side is susceptible to overfitting due to the limited sample size. Furthermore, training a global denoising model through FedAvg (McMahan et al. 2017) struggles to fully capture each local client's noisy label information, potentially leading to the incorrect identification of noisy samples. Moreover, existing methods designed for FL with heterogeneous noisy labels can be broadly categorized into coarse-grained and fine-grained methods. However, these methods often fail to effectively recognize wrongly labeled samples from different clients, resulting in significant performance discrepancies (Tuor et al. 2021; Xu et al. 2022).

Contributions:

Our contributions can be summarized as follows:

Novel Dual-Structure Model: We propose a novel dual-structure model specifically designed to address the challenging scenario of FL with heterogeneous label noise. By leveraging this dual-structure approach, our method allows better adaptation to client-specific noisy label distributions, significantly improving the overall performance and robustness of FL models. To prevent overfitting of the personalized model due to limited samples, we introduce a regularization term that effectively constrains the distance between the two models.

Confidence Regularized (CR) Loss: We introduce the Confidence Regularized (CR) loss, a novel loss function that plays a crucial role in effectively distinguishing between noisy and clean label samples during the training process of the dual model. This loss function enhances the model's ability to filter out noisy labels, leading to improved generalization and performance.

Extensive Experimental Validation: Our proposed method, FedFixer, is subjected to extensive experiments to demonstrate its effectiveness in addressing heterogeneous label noise across different clients. Through rigorous evaluations on diverse datasets, we show that FedFixer significantly outperforms other methods in FL settings, particularly when dealing with a high number of noisy samples. These experimental results validate the practical applicability and superiority of FedFixer in real-world FL scenarios.