Federated Learning (FL) aims to learn a common model from different clients while maintaining client data privacy and it has gradually been applied to real applications (Li et al. 2020b; He et al. 2020, 2019). However, data in each local client may be biased (Li et al. 2019, 2021b; Zhu et al. 2021) and noisy, which severely degrades the generalization performance of FL models (Wang et al. 2022).

Unlike Centralized Learning (CL) with noisy labels, FL faces the challenge of heterogeneous noisy label distributions across clients. The heterogeneity in the noisy label distribution is influenced by two primary factors: variations in the true label distribution across clients and personalized human labeling errors among them (Wei et al. 2022; Han et al. 2018; Yi et al. 2022). Figure 1(a) illustrates this concept, where we can observe significant differences in the distribution of noisy labels between client p and client q even though they have the same categories as the whole dataset on all clients. The existence of heterogeneous noisy labels in FL severely degrades the generalization performance of FL models (Wang et al. 2022). As depicted in Figure 1(b), we find that the FL model’s performance is damaged even more in the FL with heterogeneous noisy labels, compared to CL with noisy labels. Therefore, it’s crucial to devise a new label noise learning method tailored FL.

Numerous research efforts have been devoted to address noisy labels in CL (Cheng et al. 2021; Li, Socher, and Hoi 2019; Liu and Guo 2020; Natarajan et al. 2013; Wei et al. 2020). However, these CL-based methods encounter challenges when directly applied to FL. For example, learning a denoising model using local noisy label data on the client-side is easily overfit due to the limited sample size. Moreover, training a global denoising model through FedAvg (McMahan et al. 2017) can’t fully capture each local client’s noisy label information, potentially leading to the incorrect identification of noisy samples. Regarding FL with heterogeneous noisy labels, existing studies can be categorized into two types: coarse-grained methods that select clients with recognized low-noise levels, which may not effectively remove specific noisy label samples on each client (Chen et al. 2020; Li et al. 2021a; Yang et al. 2021), and fine-grained methods that identify and remove the noisy label samples on each client ignore the heterogeneous label noise distribution cross clients, resulting in significant performance differences (Tuor et al. 2021; Xu et al. 2022). It is challenging to effectively recognize wrongly labeled samples from different clients in FL.

Contributions:

Our contributions can be summarized as:

(1) We propose a novel dual-structure model to address the challenging scenario of FL with heterogeneous label noise. To prevent overfitting of the personalized model due to limited samples, we introduce a regularization term to constrain the distance between the two models.

(2) We introduce the Confidence Regularized (CR) loss, which can effectively distinguishes between noisy and clean label samples during the training process of the dual model.

(3) Extensive experiments demonstrate that our method, FedFixer, successfully identifies and addresses heterogeneous label noise across different clients. Furthermore, it strongly outperforms other methods in FL settings with a high number of noisy samples.